{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f2644d",
   "metadata": {},
   "source": [
    "Aim: Program to implement Naïve Bayes Algorithm using any standard dataset\n",
    "available in the public domain and find the accuracy of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41539f",
   "metadata": {},
   "source": [
    "\n",
    "Short notes: Naive Bayes\n",
    "\n",
    "Bayes’ Theorem provides a way that we can calculate the probability of a piece of data belonging to a given class, given our prior knowledge. Bayes’ Theorem is stated as:\n",
    "\n",
    "P(class|data) = (P(data|class) * P(class)) / P(data)\n",
    "\n",
    "Where P(class|data) is the probability of class given the provided data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f941f8",
   "metadata": {},
   "source": [
    "We are using Iris Dataset. The Iris Flower Dataset involves predicting the flower species given measurements of iris flowers.\n",
    "\n",
    "It is a multiclass classification problem. The number of observations for each class is balanced. There are 150 observations with 4 input variables and 1 output variable. The variable names are as follows:\n",
    "\n",
    "Sepal length in cm.\n",
    "\n",
    "Sepal width in cm.\n",
    "\n",
    "Petal length in cm.\n",
    "\n",
    "Petal width in cm., and\n",
    "\n",
    "Class.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "Step 1: Separate By Class.\n",
    "\n",
    "Step 2: Summarize Dataset.\n",
    "\n",
    "Step 3: Summarize Data By Class.\n",
    "\n",
    "Step 4: Gaussian Probability Density Function.\n",
    "\n",
    "Step 5: Class Probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689fe8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import neighbors, datasets, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, :], iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state = 0, train_size = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9555b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f692823e",
   "metadata": {},
   "source": [
    "In this step, we introduce the class GaussianNB that is used from the sklearn.naive_bayes library. Here, we have used a Gaussian model, there are several other models such as Bernoulli, Categorical and Multinomial. Here, we assign the GaussianNB class to the variable classifier and fit the X_train and y_train values to it for training purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17285996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15,  0,  0],\n",
       "       [ 0, 15,  0],\n",
       "       [ 0,  1, 14]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "scores = []\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test) \n",
    "scores.append(accuracy_score(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd612ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15,  0,  0],\n",
       "       [ 0, 15,  0],\n",
       "       [ 0,  1, 14]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier1 = BernoulliNB()\n",
    "classifier1.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test) \n",
    "scores.append(accuracy_score(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e0ac5",
   "metadata": {},
   "source": [
    "From the above confusion matrix, we infer that, out of 45 test set data, 44 were correctly classified and only 1 was incorrectly classified. This gives us a high accuracy of 97.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb1b372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier1 = CategoricalNB()\n",
    "#classifier1.fit(X_train, y_train)\n",
    "#y_pred = classifier.predict(X_test) \n",
    "#scores.append(accuracy_score(y_test, y_pred))\n",
    "#ValueError: Negative values in data passed to CategoricalNB (input X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0174d78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9777777777777777, 0.9777777777777777]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f0a66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
